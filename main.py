import os
import json
import requests
from bs4 import BeautifulSoup
import pandas as pd

def send_messages(webhook_url, entries):
    MAX_LEN = 1900
    messages = []
    current_msg = "**ğŸ†• æ–°ç€å…¬å‹Ÿæƒ…å ±**\n\n"

    for title, link in entries:
        line = f"ğŸ”¹ {title}\n{link}\n\n"
        if len(current_msg) + len(line) > MAX_LEN:
            messages.append(current_msg)
            current_msg = "**ğŸ†• æ–°ç€å…¬å‹Ÿæƒ…å ± ç¶šã**\n\n"
        current_msg += line

    if current_msg.strip():
        messages.append(current_msg)

    success_all = True
    for idx, msg in enumerate(messages, 1):
        res = requests.post(webhook_url, json={"content": msg})
        if res.status_code == 204:
            print(f"âœ… Discordé€šçŸ¥å®Œäº†ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸{idx}ï¼‰")
        else:
            print(f"âš ï¸ Discordé€šçŸ¥å¤±æ•—ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸{idx}ï¼‰: {res.status_code}")
            success_all = False
    return success_all

def jia_parser(url):
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    results = []
    for article in soup.find_all("article"):
        a_tag = article.find("a", href=True)
        h2_tag = article.find("h2")
        if a_tag and h2_tag:
            title = h2_tag.get_text(strip=True)
            link = a_tag["href"]
            if not link.startswith("http"):
                link = requests.compat.urljoin(url, link)
            results.append((title, link))
    return results

def mlit_parser(url):
    res = requests.get(url)
    res.encoding = res.apparent_encoding
    soup = BeautifulSoup(res.text, "html.parser")
    results = []
    items = soup.select("ul.js-pullDownFilterContents li.js-pullDownFilterContentsItem")
    for item in items:
        status_span = item.select_one("span.st-news-list__tag")
        if status_span and "å‹Ÿé›†ä¸­" in status_span.text:
            link_tag = item.find("a", href=True)
            if not link_tag:
                continue
            link = link_tag["href"]
            if not link.startswith("http"):
                link = requests.compat.urljoin(url, link)
            title_p = item.select_one("p")
            title = title_p.text.strip() if title_p else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
            results.append((title, link))
    return results

def generic_parser(url, item_selector, title_selector, link_selector):
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    results = []
    for item in soup.select(item_selector):
        title_elem = item.select_one(title_selector)
        link_elem = item.select_one(link_selector)
        if not title_elem or not link_elem:
            continue
        title = title_elem.get_text(strip=True)
        link = link_elem.get("href")
        if link and not link.startswith("http"):
            link = requests.compat.urljoin(url, link)
        results.append((title, link))
    return results

def main():
    webhook_url = os.getenv("DISCORD_WEBHOOK_URL")
    if not webhook_url:
        print("â—DISCORD_WEBHOOK_URLãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    df = pd.read_csv("sites_list.csv")
    all_results = []
    for _, row in df.iterrows():
        print(f"ğŸ“¡ {row['ã‚µã‚¤ãƒˆå']} ã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        parser_type = row["ãƒ‘ãƒ¼ã‚µãƒ¼ã‚¿ã‚¤ãƒ—"]
        url = row["URL"]

        if parser_type == "jia_parser":
            results = jia_parser(url)
            print(f"JIAãƒ‘ãƒ¼ã‚µãƒ¼å–å¾—ä»¶æ•°: {len(results)}")
        elif parser_type == "mlit_parser":
            results = mlit_parser(url)
            print(f"è¦³å…‰åºãƒ‘ãƒ¼ã‚µãƒ¼å–å¾—ä»¶æ•°: {len(results)}")
            for title, link in results:
                print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        elif parser_type == "generic":
            results = generic_parser(url, row["item_selector"], row["title_selector"], row["link_selector"])
            print(f"æ±ç”¨ãƒ‘ãƒ¼ã‚µãƒ¼å–å¾—ä»¶æ•°: {len(results)}")
        else:
            print(f"âš ï¸ æœªçŸ¥ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã‚¿ã‚¤ãƒ—: {parser_type}")
            results = []

        print(f"  â†’ {len(results)} ä»¶å–å¾—")
        all_results.extend(results)

    posted_file = "posted.json"
    if os.path.exists(posted_file):
        with open(posted_file, "r", encoding="utf-8") as f:
            posted_urls = set(json.load(f))
    else:
        posted_urls = set()

    new_entries = [(t, l) for t, l in all_results if l not in posted_urls]
    if not new_entries:
        print("â„¹ï¸ æ–°ã—ã„æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    if send_messages(webhook_url, new_entries):
        print("Discordé€šçŸ¥æˆåŠŸã€‚posted.jsonã‚’æ›´æ–°ã—ã¾ã™ã€‚")
        try:
            with open(posted_file, "w", encoding="utf-8") as f:
                json.dump(list(posted_urls.union([link for _, link in new_entries])), f, ensure_ascii=False, indent=2)
            print("âœ… posted.jsonã‚’æ­£å¸¸ã«æ›´æ–°ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âŒ posted.jsonã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    else:
        print("Discordé€šçŸ¥å¤±æ•—ã€‚posted.jsonã®æ›´æ–°ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

if __name__ == "__main__":
    main()
