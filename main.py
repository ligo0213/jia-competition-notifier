import os
import json
import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urlparse, urlunparse
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

def send_messages(webhook_url, site_entries_dict, bot_name="å…¬å‹Ÿæƒ…å ±"):
    MAX_LEN = 1900
    messages = []
    current_msg = "**æ–°ç€æƒ…å ±**\n\n"

    for site_name, entries in site_entries_dict.items():
        current_msg += f"â—‡{site_name}\n"
        for title, link in entries:
            line = f"ãƒ»{title}\n{link}\n"
            if len(current_msg) + len(line) > MAX_LEN:
                messages.append(current_msg)
                current_msg = ""  # ç¶šãã‚¿ã‚¤ãƒˆãƒ«ãªã—
            current_msg += line
        current_msg += "\n"

    if current_msg.strip():
        messages.append(current_msg)

    success_all = True
    for idx, msg in enumerate(messages, 1):
        res = requests.post(webhook_url, json={"content": msg, "username": bot_name})
        if res.status_code == 204:
            print(f"âœ… Discordé€šçŸ¥å®Œäº†ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸{idx}ï¼‰")
        else:
            print(f"âš ï¸ Discordé€šçŸ¥å¤±æ•—ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸{idx}ï¼‰: {res.status_code}")
            success_all = False
    return success_all

def normalize_url(url):
    parsed = urlparse(url)
    clean = parsed._replace(fragment="", query="")
    return urlunparse(clean)

def requests_retry_session(retries=3, backoff_factor=0.5, status_forcelist=(500, 502, 504)):
    session = requests.Session()
    retry = Retry(
        total=retries,
        read=retries,
        connect=retries,
        backoff_factor=backoff_factor,
        status_forcelist=status_forcelist,
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('https://', adapter)
    session.mount('http://', adapter)
    return session

def jia_parser(url):
    session = requests_retry_session()
    try:
        res = session.get(url)
        soup = BeautifulSoup(res.text, "html.parser")
        results = []
        for article in soup.find_all("article"):
            a_tag = article.find("a", href=True)
            h2_tag = article.find("h2")
            if a_tag and h2_tag:
                title = h2_tag.get_text(strip=True)
                link = a_tag["href"]
                if not link.startswith("http"):
                    link = requests.compat.urljoin(url, link)
                results.append((title, link))
        return results
    except Exception as e:
        print(f"âš ï¸ JIAãƒ‘ãƒ¼ã‚µãƒ¼ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return []

def mlit_parser(url):
    session = requests_retry_session()
    try:
        res = session.get(url)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        results = []
        items = soup.select("ul.js-pullDownFilterContents li.js-pullDownFilterContentsItem")
        for item in items:
            status_span = item.select_one("span.st-news-list__tag")
            if status_span and "å‹Ÿé›†ä¸­" in status_span.text:
                link_tag = item.find("a", href=True)
                if not link_tag:
                    continue
                link = link_tag["href"]
                if not link.startswith("http"):
                    link = requests.compat.urljoin(url, link)
                title_p = item.select_one("p")
                title = title_p.text.strip() if title_p else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
                results.append((title, link))
        return results
    except Exception as e:
        print(f"âš ï¸ è¦³å…‰åºãƒ‘ãƒ¼ã‚µãƒ¼ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return []

def mext_parser(url):
    session = requests_retry_session()
    try:
        res = session.get(url)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        results = []

        for dl in soup.find_all("dl"):
            dt = dl.find("dt")
            dd = dl.find("dd")
            if dt and dd:
                a = dd.find("a", href=True)
                if a:
                    title = a.get_text(strip=True)
                    link = a['href']
                    if not link.startswith("http"):
                        link = requests.compat.urljoin(url, link)
                    results.append((title, link))
        return results
    except Exception as e:
        print(f"âš ï¸ æ–‡ç§‘çœãƒ‘ãƒ¼ã‚µãƒ¼ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return []

def tokyo_kosha_parser(url):
    session = requests_retry_session()
    try:
        res = session.get(url)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        results = []

        for a_tag in soup.select("a.bl_support_item"):
            status_div = a_tag.find("div", class_="un_josei_status")
            if status_div and "å‹Ÿé›†ä¸­" in status_div.text:
                title_div = a_tag.find("div", class_="bl_support_item_ttl")
                if title_div:
                    title = title_div.get_text(strip=True)
                else:
                    title = "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
                link = a_tag.get("href")
                if link and not link.startswith("http"):
                    link = requests.compat.urljoin(url, link)
                results.append((title, link))
        return results
    except Exception as e:
        print(f"âš ï¸ æ±äº¬éƒ½ä¸­å°ä¼æ¥­æŒ¯èˆˆå…¬ç¤¾ãƒ‘ãƒ¼ã‚µãƒ¼ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return []

def artscouncil_tokyo_parser(url):
    session = requests_retry_session()
    try:
        res = session.get(url)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        results = []

        sections = soup.select("section.box_harf_02.box_harf_02--support")
        for section in sections:
            title_tag = section.select_one("h2")
            link_tag = section.select_one("a[href]")
            if title_tag and link_tag:
                title = title_tag.get_text(strip=True)
                link = link_tag["href"]
                if not link.startswith("http"):
                    link = requests.compat.urljoin(url, link)
                results.append((title, link))
        return results
    except Exception as e:
        print(f"âš ï¸ ã‚¢ãƒ¼ãƒ„ã‚«ã‚¦ãƒ³ã‚·ãƒ«æ±äº¬ãƒ‘ãƒ¼ã‚µãƒ¼ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return []

def tokyo_artscouncil_grant_parser(url):
    session = requests_retry_session()
    try:
        res = session.get(url)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        results = []

        sections = soup.select("section.box_harf_02.box_harf_02--support")
        for section in sections:
            title_tag = section.select_one("h2")
            link_tag = section.select_one("a[href]")
            if title_tag and link_tag:
                title = title_tag.get_text(strip=True)
                link = link_tag["href"]
                if not link.startswith("http"):
                    link = requests.compat.urljoin(url, link)
                results.append((title, link))
        return results
    except Exception as e:
        print(f"âš ï¸ æ±äº¬ã‚¢ãƒ¼ãƒ„ã‚«ã‚¦ãƒ³ã‚·ãƒ«åŠ©æˆãƒ‘ãƒ¼ã‚µãƒ¼ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return []

def canpan_parser(url):
    session = requests_retry_session()
    try:
        res = session.get(url)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        results = []

        rows = soup.select("table tbody tr")
        for row in rows:
            title_tag = row.select_one("h3 a")
            org_tag = row.select_one("dd p a")
            status_tag = row.select_one("p.status")

            if status_tag and "å‹Ÿé›†ä¸­" in status_tag.text:
                title = title_tag.get_text(strip=True) if title_tag else "ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜"
                org = org_tag.get_text(strip=True) if org_tag else "å®Ÿæ–½å›£ä½“ä¸æ˜"
                link = title_tag["href"] if title_tag else None
                if link and not link.startswith("http"):
                    link = requests.compat.urljoin(url, link)
                results.append((title, link, org))
        return results
    except Exception as e:
        print(f"âš ï¸ CANPANãƒ‘ãƒ¼ã‚µãƒ¼ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return []

def main():
    webhook_url = os.getenv("DISCORD_WEBHOOK_URL")
    if not webhook_url:
        print("â—DISCORD_WEBHOOK_URLãŒç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    df = pd.read_csv("sites_list.csv")
    site_results = {}

    for _, row in df.iterrows():
        site_name = row['ã‚µã‚¤ãƒˆå']
        parser_type = row["ãƒ‘ãƒ¼ã‚µãƒ¼ã‚¿ã‚¤ãƒ—"]
        url = row["URL"]

        if parser_type == "jia_parser":
            results = jia_parser(url)
        elif parser_type == "mlit_parser":
            results = mlit_parser(url)
        elif parser_type == "mext_parser":
            results = mext_parser(url)
        elif parser_type == "tokyo_kosha_parser":
            results = tokyo_kosha_parser(url)
        elif parser_type == "artscouncil_tokyo_parser":
            results = artscouncil_tokyo_parser(url)
        elif parser_type == "tokyo_artscouncil_grant_parser":
            results = tokyo_artscouncil_grant_parser(url)
        elif parser_type == "canpan_parser":
            results = canpan_parser(url)
            # canpan_parserã¯org(å®Ÿæ–½å›£ä½“)ã‚‚è¿”ã™ã®ã§site_resultsã«ç‰¹åˆ¥ä¿å­˜
            # ã“ã“ã¯ã‚¿ãƒ—ãƒ«ã®å½¢ãŒç•°ãªã‚‹ã®ã§åˆ†ã‘ã¦ä¿æŒã™ã‚‹ã‹èª¿æ•´ãŒå¿…è¦
            # ã¾ãšã¯æ™®é€šã«append
            site_results[site_name] = site_results.get(site_name, []) + [(t, l) for t, l, o in results]
            print(f"ğŸ“¡ {site_name} ã®æƒ…å ±ã‚’å–å¾—ä¸­â€¦ {len(results)} ä»¶")
            continue
        elif parser_type == "generic":
            results = generic_parser(url, row["item_selector"], row["title_selector"], row["link_selector"])
        else:
            print(f"âš ï¸ æœªçŸ¥ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã‚¿ã‚¤ãƒ—: {parser_type}")
            results = []

        print(f"ğŸ“¡ {site_name} ã®æƒ…å ±ã‚’å–å¾—ä¸­â€¦ {len(results)} ä»¶")
        site_results[site_name] = site_results.get(site_name, []) + results

    posted_file = "posted.json"
    if os.path.exists(posted_file):
        with open(posted_file, "r", encoding="utf-8") as f:
            posted_urls = set(normalize_url(u) for u in json.load(f))
    else:
        posted_urls = set()

    filtered_results = {}
    for site_name, entries in site_results.items():
        filtered = [(t, l) for t, l in entries if normalize_url(l) not in posted_urls]
        if filtered:
            filtered_results[site_name] = filtered

    if not filtered_results:
        print("â„¹ï¸ æ–°ã—ã„æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return

    if send_messages(webhook_url, filtered_results, bot_name="å…¬å‹Ÿæƒ…å ±"):
        print("Discordé€šçŸ¥æˆåŠŸã€‚posted.jsonã‚’æ›´æ–°ã—ã¾ã™ã€‚")
        all_new_urls = [normalize_url(link) for entries in filtered_results.values() for _, link in entries]
        posted_urls.update(all_new_urls)
        try:
            with open(posted_file, "w", encoding="utf-8") as f:
                json.dump(list(posted_urls), f, ensure_ascii=False, indent=2)
            print("âœ… posted.jsonã‚’æ­£å¸¸ã«æ›´æ–°ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âŒ posted.jsonã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    else:
        print("Discordé€šçŸ¥å¤±æ•—ã€‚posted.jsonã®æ›´æ–°ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

if __name__ == "__main__":
    main()
